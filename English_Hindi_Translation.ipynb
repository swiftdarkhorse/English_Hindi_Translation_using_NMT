{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IITB_Internship_Assignment3_Code.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rccXADX6rTju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7409a4d8-d84f-4f95-f37e-c1febbd475f7"
      },
      "source": [
        "import pandas as pd\n",
        "import codecs\n",
        "import numpy as np\n",
        "import xlrd\n",
        "import re\n",
        "import unicodedata\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import time\n",
        "import os\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Activation, Permute\n",
        "from keras.layers import Input, Flatten, Dropout\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import TimeDistributed, Bidirectional"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQLl1vVeroqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading of Excel into a Pandas Dataframe and then further converting it into a Numpy Array\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    #file_path = '/content/drive/My Drive/Colab Notebooks/Hi-En-Parallel_Corpus.xlsx'\n",
        "\n",
        "    #Read Excel file using xlrd \n",
        "    book = xlrd.open_workbook(file_path, encoding_override='utf-8')\n",
        "    #Convert Excel into Pandas Dataframe\n",
        "    df = pd.read_excel(book)\n",
        "    #Convert Pandas dataframe into Numpy Array\n",
        "    dataset = np.array(df)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3ecV7mPx4Ud",
        "colab_type": "code",
        "outputId": "1f50250e-a179-402c-e7ec-1216b11ea2ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "#Taking the Excel File Name from the user and displaying it as a Numpy Array\n",
        "\n",
        "file_name = input(\"Enter the name of the Excel file containing mapping from Input Language to Target Language\\n\")\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/\" + str(file_name)\n",
        "dataset = load_dataset(file_path)\n",
        "print(\"The dataset after loading in Array is:\")\n",
        "print(dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the name of the Excel file containing mapping from Input Language to Target Language\n",
            "Hi-En-Parallel_Corpus.xlsx\n",
            "The dataset after loading in Array is:\n",
            "[['politicians do not have permission to do what needs to be done.'\n",
            "  'राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .']\n",
            " [\"I'd like to tell you about one such child,\"\n",
            "  'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,']\n",
            " ['This percentage is even greater than the percentage in India.'\n",
            "  'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।']\n",
            " ...\n",
            " [\"As for the other derivatives of sulphur , the country 's needs of iron , copper , sodium , etc . sulphates were limited , and the production achieved after the war was generally adequate .\"\n",
            "  'जहां तक गंधक के अन्य उत्पादों का प्रश्न है , देश में लोहे , तांबे , सोडियम , सल्फेट आदि की आवश्यकता सीमित थी और युद्धोपरांत हुआ उत्पादन सामान्य रूप से पर्याप्त था .']\n",
            " ['its complicated functioning is defined thus in a popular riddle :'\n",
            "  'Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .']\n",
            " [\"They've just won four government contracts to build off their 100 ambulances,\"\n",
            "  'हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ नई अम्बुलेन्स बनाने का,']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1BY9vNCswx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing the Dataset \n",
        "#1. Converting to lowercase\n",
        "#2. Removing all the special characters\n",
        "#3. Removing all numbers from text\n",
        "#4. Adding Start and End Tags\n",
        "\n",
        "def data_preprocessing(dataset):\n",
        "    i=0\n",
        "    for i in range(dataset.shape[0]):\n",
        "        #Converting all sentences into lower case\n",
        "        dataset[i,0] = str(dataset[i,0]).lower()\n",
        "        dataset[i,1] = str(dataset[i,1]).lower()\n",
        "        #Removing all special characters from the sentences\n",
        "        temp = dataset[i,0]\n",
        "        dataset[i,0] = \"\"\n",
        "        for k in temp:\n",
        "            dataset[i,0] += re.sub(r\"[^a-zA-Z0-9]+\", ' ', k)\n",
        "        #Removing all numbers from the sentence\n",
        "        dataset[i,0] = re.sub(r'\\d+', '', dataset[i,0])\n",
        "        dataset[i,1] = re.sub(r'\\d+', '', dataset[i,1])\n",
        "        #Attaching Start and End Tags\n",
        "        dataset[i,0] = \"<start> \" + dataset[i,0] + \" <end>\"\n",
        "        dataset[i,1] = \"<start> \" + dataset[i,1] + \" <end>\"\n",
        "        #Eliminating extra spaces from tokens\n",
        "        dataset[i,0] = re.sub(' +', ' ',dataset[i,0])\n",
        "        dataset[i,1] = re.sub(' +', ' ',dataset[i,1]) \n",
        "        #dataset[i,0] = str(dataset[i,0].encode('utf-8'))\n",
        "        #dataset[i,1] = str(dataset[i,1].encode('utf-8'))\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-kN04k5y1lI",
        "colab_type": "code",
        "outputId": "29536cb1-11eb-4b6f-e5e0-144801574825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Implementing Preprocessing on the Dataset\n",
        "\n",
        "dataset = data_preprocessing(dataset)\n",
        "print(\"The dataset after data preprocessing:\")\n",
        "print(dataset)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset after data preprocessing:\n",
            "[['<start> politicians do not have permission to do what needs to be done <end>'\n",
            "  '<start> राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है . <end>']\n",
            " ['<start> i d like to tell you about one such child <end>'\n",
            "  '<start> मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी, <end>']\n",
            " ['<start> this percentage is even greater than the percentage in india <end>'\n",
            "  '<start> यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है। <end>']\n",
            " ...\n",
            " ['<start> as for the other derivatives of sulphur the country s needs of iron copper sodium etc sulphates were limited and the production achieved after the war was generally adequate <end>'\n",
            "  '<start> जहां तक गंधक के अन्य उत्पादों का प्रश्न है , देश में लोहे , तांबे , सोडियम , सल्फेट आदि की आवश्यकता सीमित थी और युद्धोपरांत हुआ उत्पादन सामान्य रूप से पर्याप्त था . <end>']\n",
            " ['<start> its complicated functioning is defined thus in a popular riddle <end>'\n",
            "  '<start> zरचना-प्रकिया को उसने एक पहेली में यों बांधा है . <end>']\n",
            " ['<start> they ve just won four government contracts to build off their ambulances <end>'\n",
            "  '<start> हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ नई अम्बुलेन्स बनाने का, <end>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLQkIR8Ms3IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting the Input and Target Language Statements from the Database\n",
        "\n",
        "def input_target_split(dataset):\n",
        "    #Store first column of Dataset as Input Language\n",
        "    input_lang = dataset[:,0]\n",
        "    #Store second column of Dataset as Target Language\n",
        "    target_lang = dataset[:,1]\n",
        "    return input_lang, target_lang"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d93kXxP8zX4l",
        "colab_type": "code",
        "outputId": "b0210cd9-ee43-469c-f8a4-96502fa5779e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "#Implementing the split on the Dataset\n",
        "\n",
        "input_lang, target_lang = input_target_split(dataset)\n",
        "print(\"The input language of dataset is:\")\n",
        "print(input_lang)\n",
        "print(\"\\n\")\n",
        "print(\"The target language of dataset is:\")\n",
        "print(target_lang)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input language of dataset is:\n",
            "['<start> politicians do not have permission to do what needs to be done <end>'\n",
            " '<start> i d like to tell you about one such child <end>'\n",
            " '<start> this percentage is even greater than the percentage in india <end>'\n",
            " ...\n",
            " '<start> as for the other derivatives of sulphur the country s needs of iron copper sodium etc sulphates were limited and the production achieved after the war was generally adequate <end>'\n",
            " '<start> its complicated functioning is defined thus in a popular riddle <end>'\n",
            " '<start> they ve just won four government contracts to build off their ambulances <end>']\n",
            "\n",
            "\n",
            "The target language of dataset is:\n",
            "['<start> राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है . <end>'\n",
            " '<start> मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी, <end>'\n",
            " '<start> यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है। <end>' ...\n",
            " '<start> जहां तक गंधक के अन्य उत्पादों का प्रश्न है , देश में लोहे , तांबे , सोडियम , सल्फेट आदि की आवश्यकता सीमित थी और युद्धोपरांत हुआ उत्पादन सामान्य रूप से पर्याप्त था . <end>'\n",
            " '<start> zरचना-प्रकिया को उसने एक पहेली में यों बांधा है . <end>'\n",
            " '<start> हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ नई अम्बुलेन्स बनाने का, <end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFVB4cnXpmuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initializing the Tokenizer along with padding\n",
        "\n",
        "def initialize_tokenizer(lang):\n",
        "    #Initialize Tokenizer\n",
        "    lang_tokenizer = Tokenizer(filters='')\n",
        "    #Fit the Tokenizer on the text\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    #Convert the sentences into sequence of corresponding indices\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    #Pad the senquences to make them of equal length\n",
        "    max_sequence_len = max([len(x) for x in tensor])\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen = max_sequence_len, padding='post')\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94WYpfIMqEZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenizing the statements using the initialized tokenizer\n",
        "\n",
        "def tokenize(input_lang, target_lang):\n",
        "    #Tokenize the input language\n",
        "    input_tensor, inp_lang_tokenizer = initialize_tokenizer(input_lang)\n",
        "    #Tokenize the target language\n",
        "    target_tensor, targ_lang_tokenizer = initialize_tokenizer(target_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUxnr5qa2JDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Tokenize the input and target languages\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = tokenize(input_lang, target_lang)\n",
        "#Split the dataset into train and set\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_1QQ8mQxNpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mapping of the Tokenized words to their Indices\n",
        "\n",
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t!=0:\n",
        "           print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBN9FbJ1-bwK",
        "colab_type": "code",
        "outputId": "394529fb-8d6e-4a53-f47f-30799affada8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "#Displaying the Tokenized words and Word Indices mappings\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print (\"\\n\")\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "2 ----> <start>\n",
            "80 ----> because\n",
            "22 ----> he\n",
            "19 ----> s\n",
            "61 ----> been\n",
            "369 ----> able\n",
            "6 ----> to\n",
            "551 ----> understand\n",
            "10 ----> that\n",
            "22 ----> he\n",
            "3 ----> <end>\n",
            "\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "154 ----> क्योंकि\n",
            "39 ----> वह\n",
            "18 ----> यह\n",
            "1084 ----> समझने\n",
            "4 ----> में\n",
            "1446 ----> सक्षम\n",
            "26 ----> हो\n",
            "34 ----> गया\n",
            "15 ----> कि\n",
            "221 ----> उसने\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VI4m8XdzUxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initializing necessary utilities for model building and training\n",
        "\n",
        "input_word_to_index = inp_lang.word_index\n",
        "target_word_to_index = targ_lang.word_index\n",
        "input_index_to_word = dict([(value, key) for key, value in inp_lang.word_index.items()])\n",
        "target_index_to_word = dict([(value, key) for key, value in targ_lang.word_index.items()])\n",
        "max_len_input = (input_tensor.shape[1])\n",
        "max_len_target = (target_tensor.shape[1])\n",
        "input_vocab_size = len(inp_lang.word_index)\n",
        "target_vocab_size = len(targ_lang.word_index)\n",
        "rnn_cell_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m49UjcHCDvd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initializing necessary utilities for model building and training\n",
        "\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-ZHG6ShZQP5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "945fb725-189e-4d01-c92f-0710e7573420"
      },
      "source": [
        "#Initializaing Sample Batches to Display their Shapes\n",
        "\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 400]), TensorShape([64, 420]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAsZGVh-ZpF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the Encoder Class for the Attention Model of RNN\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2eDCha1ZubE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "38be65bf-dbae-48fe-8dc1-b3629896b16b"
      },
      "source": [
        "#Assigning the Encoder class with an object\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 400, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqN3yJ4MZ2Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the Bahdanau Attention Layer Class for the Attention Model of RNN\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWKcCrdeZ4sf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "d642d7de-ef6a-4e26-f974-d131154eb225"
      },
      "source": [
        "#Assigning the Bahdanau Attention Layer class with an object\n",
        "\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 400, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1K9ws1gaBjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Building the Decoder Class for the Attention Model of RNN\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2y7qjZQaFW_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57e1510d-3b6c-4599-863a-b5787a4a25e0"
      },
      "source": [
        "#Assigning the Decoder class with an object\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 89734)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXuz4Z8IaNad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initializing the Optimizer and Loss Function for the Model Training\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T7_xkk_aTm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initializing the checkpoints in case the the training needs to be continued in future\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DooKnrLnalf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Performing the training on the Model on one Epoch\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0yPDjf5arUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The Model training with defining the No. of Epochs the Inputs and Labels\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZPXCMk8bj1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluating the Trained Model over Test Sentence\n",
        "\n",
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqQcL2dNbnhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for plotting the attention weights\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcspDmyGbpe6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Translating the sentence from Input Language to Target Language\n",
        "\n",
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgZrTzWIbxxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sybmJb_mby4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Translating the statement giving the statement in the input language\n",
        "\n",
        "translate('I love to play football.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12SU-cBodE2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Computing the Bleu Score by Comparing the Statement Translated by the Model and Human/Provided Translation\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "reference = [word_tokenize(translate(input_lang[100]))]\n",
        "candidate = word_tokenize(target_lang[100])\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(score)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
